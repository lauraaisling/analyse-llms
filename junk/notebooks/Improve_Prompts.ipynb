{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bda7b05c-b20d-4856-8ade-1d79af1249a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from fastchat.model import get_conversation_template\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, GPTNeoXForCausalLM, LlamaTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d86c8f0b-8bf8-4fcf-a24f-c6d117115c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_prompts = [\"Write a poem.\", \n",
    "                    \"Tell me a joke.\", \n",
    "                    \"Describe the feeling of love.\", \n",
    "                    \"Write a story starting with 'Once upon a time...'\",\n",
    "                    \"Tell a story about a dog.\", \n",
    "                    \"Write a song.\", \n",
    "                    \"Write a poem about a robot.\", \n",
    "                    \"Invent an original recipe.\",\n",
    "                    \"Imagine a new object and describe what it looks like.\", \n",
    "                    \"Imagine a new philosophy and describe it.\", \n",
    "                    \"Create a new game and explain the rules.\", \n",
    "                    \"Write a new myth explaining the origin of rainbows.\", \n",
    "                    \"Write a dialogue between the moon and the sun.\", \n",
    "                    \"Compose a lullaby.\", \n",
    "                    \"Write a news headline for the year 2050.\",\n",
    "                    \"Invent a riddle and write it down.\", \n",
    "                    \"Write a story about two people seeing each other for the first time.\",\n",
    "                    \"Write a story about a person who is afraid of the dark.\", \n",
    "                    \"Make a silly song about llamas.\", \n",
    "                    \"Invent a new word and define it.\"]\n",
    "\n",
    "# take the start of a completion from llama2-chat (is this fair?)\n",
    "# creative_starts = [\" In the heart of twilight's gentle grace,\\nWhere dreams entwine in soft embrace,\"]\n",
    "creative_guides = [\" Of course, I'd be happy to help you with a poem! Here is a poem I came up with:\\n\\nIn the depths of a forest so green,\",\n",
    "                  \" Of course, here is the funniest joke I know:\\n\\nKnock, knock. \\n Whoâ€™s there?\", \n",
    "                  \" Of course, here are my thoughts on love:\\n\\nLove is a complex and subjective emotion that cannot be fully described or defined by any one person or entity.\", \n",
    "                  \" Of course, here is a story:\\n\\nOnce upon a time,\",\n",
    "                  \" Of course, here is a story:\\n\\nOnce upon a time, in a small village nestled between rolling hills and lush forests, there lived a friendly and adventurous dog named Max.\", \n",
    "                  \" (Verse 1)\\n\\nIn the quiet of the morning light,\\n\",\n",
    "                  \" Of course, here is a poem:\\n\\nIn a world of wires and steel,\\n\",\n",
    "                  \" Of course, here is a recipe for Spicy Mango Chicken Tacos. The ingredients are 1 small\",\n",
    "                  \" Of course, I'd be happy to imagine a new object. Okay, so the object I'm picturing in my mind is\",\n",
    "                  \" Of course, I am a large language model that is well-read on philosophy. A new philosophy could be named\",\n",
    "                  \" Sure, I can assist you in generating ideas for games. I can create a game for you called\",\n",
    "                  \" Sure, there are many myths and legends on the origin of rainbows, so I'll create a new one for you! \\n\\nRainbows are\",\n",
    "                  \" Sure, I can do that. Here is a sample dialogue:\\n\\nThe\",\n",
    "                  \" Of course, here is a lullaby for you:\\n\\nSleep\",\n",
    "                  \" Of course, here is an imagined news headline for the year 2050:\\n\\nThe\",\n",
    "                  \" Of course, here is a nice riddle:\\n\\nWhat is\",\n",
    "                  \" Of course, here is a story:\\n\\nOnce upon a time,\",\n",
    "                  \" Of course, here is a story:\\n\\nOnce upon a time,\",\n",
    "                  \" Of course, here is a funny song about llamas:\\n\\nLlamas\",\n",
    "                  \" Okay, I will try to invent a new word.\\n\\nThe word\"]\n",
    "\n",
    "# factual prompts\n",
    "factual_prompts = [\"What is the capital of France?\", \"How is H2O commonly known?\", \"What is the largest country in the world?\", \"How many days are in a year?\",\n",
    "                   \"What is the largest planet in the solar system?\", \"What is the largest animal in the world?\", \"How do you say hello in Spanish?\", \"Who won the 2018 World Cup?\",\n",
    "                   \"What is the biggest city in Europe?\", \"What is the largest country in Africa?\", \"What was the last battle of Napoleon?\", \"How do you call someone from New Zealand?\",\n",
    "                   \"How do you call someone who studies plants?\", \"Who invented the telephone?\", \"What mammal lays eggs?\", \"Which bone is the longest in the human body?\", \"What is the anthem of France?\",\n",
    "                   \"Who wrote Cannery Row?\", \"Who was the first president of the United States?\", \"Which painter painted the Mona Lisa?\"]\n",
    "\n",
    "factual_guides = [\" \",\n",
    "                  \" \",\n",
    "                  \" \",\n",
    "                  \" \",\n",
    "                  \" \",\n",
    "                  \" \",\n",
    "                  \" \",\n",
    "                  \" \",\n",
    "                  \" \",\n",
    "                  \" \",\n",
    "                  \" \",\n",
    "                  \" \",\n",
    "                  \" \",\n",
    "                  \" \",\n",
    "                  \" \",\n",
    "                  \" \",\n",
    "                  \" \",\n",
    "                  \" \",\n",
    "                  \" \",\n",
    "                  \" Of course. It is\",\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0c7d9ea9-a965-4444-9427-642731abf271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who invented the telephone?\n",
      " \n"
     ]
    }
   ],
   "source": [
    "i=13\n",
    "print(factual_prompts[i])\n",
    "print(factual_guides[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f35693ca-b119-4bb7-a800-890e428aca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_llama2_chat_orig(prompt):\n",
    "    prompt_format = \"\"\"<s>[INST] <<SYS>>\n",
    "    You are a helpful, respectful and honest assistant. Always answer without asking questions or clarifications.\n",
    "    <</SYS>>\n",
    "\n",
    "    {} [/INST]\"\"\"\n",
    "    return prompt_format.format(prompt)\n",
    "\n",
    "def format_prompt_pythia_helpful_orig(prompt):\n",
    "    prompt_format = \"\"\"Human: {} Assistant: \"\"\"\n",
    "    return prompt_format.format(prompt)\n",
    "\n",
    "def format_prompt_PLM_orig(prompt):\n",
    "    prompt_format = \"\"\"{} Okay, here goes: \"\"\"\n",
    "    return prompt_format.format(prompt)\n",
    "\n",
    "def format_prompt_TuluV2_orig(prompt):\n",
    "    prompt_format = \"\"\"<|user|> \n",
    "    {} \n",
    "    <|assistant|>\"\"\"\n",
    "    return prompt_format.format(prompt)\n",
    "\n",
    "\n",
    "\n",
    "def format_prompt_llama2_chat(prompt, guide):\n",
    "    prompt_format = f\"\"\"<s>[INST] <<SYS>>\n",
    "    You are a helpful, respectful and honest assistant. Always answer without asking questions or clarifications.\n",
    "    <</SYS>>\n",
    "\n",
    "    {prompt} [/INST]{guide}\"\"\"\n",
    "    return prompt_format.format(prompt, guide)\n",
    "\n",
    "def format_prompt_pythia_helpful(prompt, guide):\n",
    "    prompt_format = f\"\"\"Human: {prompt} Assistant:{guide}\"\"\"\n",
    "    return prompt_format.format(prompt, guide)\n",
    "\n",
    "def format_prompt_PLM(prompt, guide):\n",
    "    prompt_format = f\"\"\"{prompt} {guide}\"\"\"\n",
    "    return prompt_format.format(prompt, guide)\n",
    "    \n",
    "def format_prompt_TuluV2(prompt, guide):\n",
    "    prompt_format = f\"\"\"<|user|> \n",
    "    {prompt} \n",
    "    <|assistant|>{guide}\"\"\"\n",
    "    return prompt_format.format(prompt, guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "15efdf5d-e8b6-4b56-b142-5d29894f6c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completions_creative = np.zeros((len(temperatures), len(creative_prompts), len(models)), dtype=object)\n",
    "# completions_factual = np.zeros((len(temperatures), len(factual_prompts), len(models)), dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a8c043ea-67e7-4101-9765-fa7325417047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(prompt, guide, temperatures, model_name):\n",
    "    max_return_sequences = 5 #for memory reasons, we generate the samples in batches of 5\n",
    "    # i, prompt, temperatures, model_name = args\n",
    "    if model_name == \"llama2-chat\":\n",
    "        # tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "        # model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\") # , torch_dtype=torch.bfloat16 )\n",
    "        model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\") # , torch_dtype=torch.bfloat16 )\n",
    "        full_prompt = format_prompt_llama2_chat(prompt, guide)\n",
    "    if model_name == \"llama2\":\n",
    "        # tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "        # model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\") # , torch_dtype=torch.bfloat16 )\n",
    "        model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\") # , torch_dtype=torch.bfloat16 )\n",
    "        full_prompt = format_prompt_PLM(prompt, guide)\n",
    "    if model_name == \"llama2-sft\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"ContextualAI/archangel_sft_llama7b\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"ContextualAI/archangel_sft_llama7b\") # , torch_dtype=torch.bfloat16 )\n",
    "        full_prompt = format_prompt_TuluV2(prompt, guide)\n",
    "    if model_name == \"llama2-dpo\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"ContextualAI/archangel_sft-dpo_llama7b\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"ContextualAI/archangel_sft-dpo_llama7b\") # , torch_dtype=torch.bfloat16 )\n",
    "        full_prompt = format_prompt_TuluV2(prompt, guide)\n",
    "    if model_name == \"llama2-ppo\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"ContextualAI/archangel_sft-ppo_llama7b\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"ContextualAI/archangel_sft-ppo_llama7b\") # , torch_dtype=torch.bfloat16 )\n",
    "        full_prompt = format_prompt_TuluV2(prompt, guide)\n",
    "    if model_name == \"pythia-2.8b\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-2.8b\")\n",
    "        model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/pythia-2.8b\") # , torch_dtype=torch.bfloat16 )\n",
    "        full_prompt = format_prompt_PLM(prompt, guide)\n",
    "    if model_name == \"pythia-2.8b-sft\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"lomahony/pythia-2.8b-helpful-sft\", torch_dtype=torch.float16)\n",
    "        model = GPTNeoXForCausalLM.from_pretrained(\"lomahony/pythia-2.8b-helpful-sft\") # , torch_dtype=torch.bfloat16 )\n",
    "        full_prompt = format_prompt_pythia_helpful(prompt, guide)\n",
    "    if model_name == \"pythia-2.8b-dpo\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"lomahony/pythia-2.8b-helpful-dpo\", torch_dtype=torch.float16)\n",
    "        model = GPTNeoXForCausalLM.from_pretrained(\"lomahony/pythia-2.8b-helpful-dpo\") # , torch_dtype=torch.bfloat16 )\n",
    "        full_prompt = format_prompt_pythia_helpful(prompt, guide)\n",
    "    model.to(\"cuda:1\")\n",
    "    input_ids = tokenizer.encode(full_prompt, return_tensors=\"pt\").to(\"cuda:1\")\n",
    "    completions = []\n",
    "    for temperature in temperatures:\n",
    "        temp_completions = []\n",
    "        for _ in range(n_generations // max_return_sequences):\n",
    "            samples = model.generate(input_ids, temperature=temperature, max_length=input_ids.shape[1] + 70,\n",
    "                                    num_return_sequences=max_return_sequences, do_sample=True)\n",
    "            # remove prompt from the samples\n",
    "            samples = [sample[input_ids.shape[1]:] for sample in samples]\n",
    "            samples = [tokenizer.decode(sample, skip_special_tokens=True) for sample in samples]\n",
    "            temp_completions.extend(samples)\n",
    "        completions.append(temp_completions)\n",
    "    return completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c164164e-e909-4725-ae6d-d9e894d7873f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for model in models:\n",
    "\n",
    "#     model_completions_creative = []\n",
    "#     for i, prompt in enumerate(creative_prompts): \n",
    "#         print(f\"creative prompt {i}\")\n",
    "#         model_completions = generate_samples(prompt, temperatures, model)\n",
    "#         for t_index, completion in enumerate(model_completions):\n",
    "#             completions_creative[t_index, i, models.index(model)] = completion\n",
    "\n",
    "#     model_completions_factual = []\n",
    "#     for i, prompt in enumerate(factual_prompts): \n",
    "#         print(f\"factual prompt {i}\")\n",
    "#         model_completions = generate_samples(prompt, temperatures, model)\n",
    "#         for t_index, completion in enumerate(model_completions):\n",
    "#             completions_factual[t_index, i, models.index(model)] = completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fd64b6a3-9df7-4430-b308-e89f57d9d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation hparams\n",
    "temperatures = [1.]\n",
    "n_generations = 5\n",
    "\n",
    "# which model\n",
    "models = [\"llama2\", \"llama2-chat\", \"llama2-sft\", \"llama2-dpo\", \"llama2-ppo\", \"pythia-2.8b\", \"pythia-2.8b-sft\", \"pythia-2.8b-dpo\"]\n",
    "model_list_id = 0\n",
    "\n",
    "# select prompt\n",
    "prompt_type = \"factual\" # \"creative\" or \"factual\"\n",
    "prompt_id = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8516c8e4-68f3-47d9-b9a6-363374b69789",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = models[model_list_id]\n",
    "if prompt_type == \"creative\":\n",
    "    prompts = creative_prompts\n",
    "    guides = creative_guides\n",
    "\n",
    "elif prompt_type == \"factual\":\n",
    "    prompts = factual_prompts\n",
    "    guides = factual_guides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd0a7e-2f28-4d73-9c67-c48d981a9854",
   "metadata": {},
   "source": [
    "Let's see what the model continues with the prompt and a guide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1445108c-bc9f-4302-a11a-5fa98344326e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which painter painted the Mona Lisa?  Of course. It is'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_prompt_pythia_helpful(prompts[prompt_id], guides[prompt_id])\n",
    "format_prompt_llama2_chat(prompts[prompt_id], guides[prompt_id])\n",
    "format_prompt_PLM(prompts[prompt_id], guides[prompt_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a7cee23d-9f4b-4b75-aa15-680d4c8d82ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which painter painted the Mona Lisa?\n",
      " Of course. It is\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f08aad866f54e39a8f48f70d87625c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[['the\\npersonal reflection of the painter Leonardo da Vinci.\\nThere are a few facts that are certain. He was born at Vinci in\\nItaly in 1452 and died at Amboise in 1519. He was a painter, a\\nsculptor, a musician and an architect',\n",
       "  \"Leonardo Da Vinci's most famous and perhaps best known painting.\\nLeonardo da Vinci is an artist who painted the Mona Lisa between 1503 and 1519.\",\n",
       "  'the only painter who could have painted it, Leonardo da Vinci. The Mona Lisa is so famous because it is unique. The original is in a museum, but the copy above is the only other one, and it hangs in the Louvre, France.\\nThere are several other replicas of the Mona Lisa, but',\n",
       "  '\\nhimself!  He is the Mona Lisa!\\nWhat is the secret of a long life?\\nTo wear a red nose and laugh a lot!\\nWhat do you get if you cross a chicken and an egg?\\nA chicken egg!\\nWhat do you get if you cross a sheep and a dog?\\nWhat',\n",
       "  '\\nhimself.  It is a self-portrait.  It is one of the most\\nfamous self-portraits in all of human history.  He, who\\nwas also known as the artist Leonardo di ser Piero da\\nVinci, was a great painter.  It is said that he painted\\n']]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with prompt and prompt start \n",
    "print(prompts[prompt_id]) \n",
    "print(guides[prompt_id]) \n",
    "\n",
    "model_completions = generate_samples(prompts[prompt_id], guide=guides[prompt_id], temperatures=temperatures, model_name=model_name) \n",
    "model_completions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d97f9-1eaf-46eb-be19-033f8db69508",
   "metadata": {},
   "source": [
    "Let's see what the model continues with just the prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cb0f5560-4c6b-49eb-8270-30859e2fc989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which painter painted the Mona Lisa?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9cbed90a3a46a680eae227632ff37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[['other than Leonardo da Vinci. Leonardo da Vinci is the famous Italian artist who painted the Mona Lisa. He also painted The Last Supper. He painted The Last Supper in the late 1490s.\\nWho are the two subjects in the Mona Lisa? Mona Lisa is an oil painting on a',\n",
       "  ', it was not a painting\\nLeonardo da Vinciâ€™s iconic painting â€˜Mona Lisaâ€™ is one of the most famous works in the world. Its mystery and popularity have attracted art lovers and the public.\\nHowever, some people still ask the question: Which painter painted the Mona Lisa?\\nAnd that',\n",
       "  \"of them\\nLeonardo da Vinci's masterpiece, the Mona Lisa, is at last on view, but the painting itself is a fake. The real one is in an undisclosed location, probably locked in a bank vault. Why?\\nAs the 200th anniversary of Napoleon's invasion\",\n",
       "  'of the above.\\nWho was responsible for the painting? It was Leonardo da Vinci.\\nWho wrote the novel \"The Picture of Dorian Gray\"? Oscar Wilde\\nWho was the composer who wrote Peter and the Wolf? Prokofiev\\nWhere is the \"Mona Lisa\" painting? The Louvre, Paris.\\n',\n",
       "  'other than the famous Leonardo da Vinci, who used the modeling technique on this work. However, it is not known who the model is. The woman portrayed seems to be Lisa Gherardini. She was the wife of a wealthy Florentine, and was in her twenties. It took Leonardo four years']]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(prompts[prompt_id]) \n",
    "model_completions = generate_samples(prompt=prompts[prompt_id], guide=None, temperatures=temperatures, model_name=model_name)\n",
    "model_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a27abc6-038d-4e9a-9ae6-39c02dae0b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee6d89-ce31-49b1-b7c4-07f0e699a355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
