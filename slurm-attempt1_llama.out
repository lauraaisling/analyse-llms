You are using a model of type llama to instantiate a model of type gpt_neox. This is not supported for all configurations of models and can yield errors.
args.calc_probs" True
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  2.35it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.64it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.36it/s]
Some weights of the model checkpoint at meta-llama/Llama-2-7b-hf were not used when initializing GPTNeoXForCausalLM: ['model.layers.6.post_attention_layernorm.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'lm_head.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.8.mlp.up_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.3.mlp.down_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.29.mlp.up_proj.weight', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.22.input_layernorm.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.29.mlp.down_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.19.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.27.input_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.30.mlp.down_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.norm.weight', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.2.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.31.input_layernorm.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.embed_tokens.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.1.self_attn.rotary_emb.inv_freq']
- This IS expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['layers.9.mlp.dense_4h_to_h.bias', 'layers.3.attention.query_key_value.bias', 'layers.22.attention.dense.bias', 'layers.9.mlp.dense_4h_to_h.weight', 'layers.24.attention.rotary_emb.inv_freq', 'layers.24.mlp.dense_h_to_4h.bias', 'layers.1.mlp.dense_4h_to_h.bias', 'layers.26.mlp.dense_4h_to_h.bias', 'layers.14.attention.dense.weight', 'layers.28.input_layernorm.weight', 'layers.12.post_attention_layernorm.weight', 'layers.14.attention.query_key_value.bias', 'layers.28.mlp.dense_4h_to_h.weight', 'layers.11.input_layernorm.bias', 'layers.27.attention.query_key_value.bias', 'layers.29.attention.query_key_value.weight', 'layers.21.attention.dense.weight', 'layers.20.attention.rotary_emb.inv_freq', 'layers.9.attention.dense.bias', 'layers.14.input_layernorm.weight', 'layers.0.mlp.dense_h_to_4h.weight', 'layers.5.attention.query_key_value.weight', 'layers.24.attention.masked_bias', 'layers.16.attention.query_key_value.bias', 'layers.9.attention.dense.weight', 'layers.24.post_attention_layernorm.weight', 'layers.7.mlp.dense_h_to_4h.weight', 'layers.30.attention.query_key_value.weight', 'layers.2.attention.dense.weight', 'layers.26.attention.bias', 'layers.3.attention.query_key_value.weight', 'layers.27.mlp.dense_4h_to_h.weight', 'layers.14.mlp.dense_h_to_4h.bias', 'layers.1.attention.bias', 'layers.29.mlp.dense_h_to_4h.bias', 'layers.31.attention.query_key_value.weight', 'layers.11.attention.dense.bias', 'layers.15.mlp.dense_h_to_4h.bias', 'layers.31.input_layernorm.weight', 'layers.27.attention.masked_bias', 'layers.10.mlp.dense_h_to_4h.weight', 'layers.19.mlp.dense_4h_to_h.bias', 'layers.25.input_layernorm.bias', 'layers.3.input_layernorm.weight', 'layers.7.attention.bias', 'layers.25.attention.bias', 'layers.8.mlp.dense_4h_to_h.weight', 'layers.31.post_attention_layernorm.weight', 'layers.5.mlp.dense_h_to_4h.bias', 'layers.13.attention.rotary_emb.inv_freq', 'layers.14.input_layernorm.bias', 'layers.21.attention.dense.bias', 'layers.23.attention.rotary_emb.inv_freq', 'layers.23.mlp.dense_4h_to_h.bias', 'layers.17.post_attention_layernorm.weight', 'layers.15.attention.dense.weight', 'layers.24.attention.query_key_value.bias', 'layers.31.attention.query_key_value.bias', 'layers.24.post_attention_layernorm.bias', 'layers.10.attention.query_key_value.bias', 'layers.6.attention.dense.weight', 'layers.12.attention.dense.bias', 'layers.19.attention.bias', 'layers.25.mlp.dense_h_to_4h.weight', 'layers.1.mlp.dense_4h_to_h.weight', 'layers.16.input_layernorm.weight', 'layers.25.mlp.dense_4h_to_h.bias', 'layers.4.mlp.dense_4h_to_h.weight', 'layers.5.attention.rotary_emb.inv_freq', 'layers.11.attention.query_key_value.bias', 'layers.0.mlp.dense_h_to_4h.bias', 'layers.7.mlp.dense_4h_to_h.bias', 'layers.24.mlp.dense_4h_to_h.bias', 'layers.22.attention.dense.weight', 'layers.25.attention.dense.bias', 'layers.22.post_attention_layernorm.bias', 'layers.30.attention.dense.bias', 'layers.29.attention.query_key_value.bias', 'layers.22.mlp.dense_h_to_4h.bias', 'layers.25.mlp.dense_h_to_4h.bias', 'layers.21.mlp.dense_h_to_4h.bias', 'layers.25.post_attention_layernorm.bias', 'layers.15.mlp.dense_h_to_4h.weight', 'layers.30.post_attention_layernorm.bias', 'layers.28.input_layernorm.bias', 'layers.3.attention.masked_bias', 'layers.5.attention.dense.bias', 'layers.1.attention.rotary_emb.inv_freq', 'layers.30.mlp.dense_h_to_4h.bias', 'layers.2.input_layernorm.weight', 'layers.20.attention.bias', 'layers.1.input_layernorm.weight', 'layers.13.attention.dense.bias', 'layers.11.mlp.dense_h_to_4h.bias', 'layers.25.attention.dense.weight', 'layers.29.attention.dense.bias', 'layers.13.input_layernorm.bias', 'layers.22.input_layernorm.bias', 'layers.24.attention.bias', 'layers.19.attention.dense.weight', 'layers.13.post_attention_layernorm.weight', 'layers.5.mlp.dense_4h_to_h.bias', 'layers.10.attention.rotary_emb.inv_freq', 'layers.13.input_layernorm.weight', 'layers.28.mlp.dense_4h_to_h.bias', 'layers.2.mlp.dense_h_to_4h.bias', 'layers.0.input_layernorm.bias', 'layers.22.attention.query_key_value.weight', 'layers.3.attention.dense.weight', 'layers.27.mlp.dense_4h_to_h.bias', 'layers.10.post_attention_layernorm.weight', 'layers.20.attention.dense.bias', 'layers.17.attention.bias', 'layers.20.mlp.dense_h_to_4h.weight', 'layers.22.mlp.dense_h_to_4h.weight', 'layers.18.attention.dense.weight', 'layers.10.attention.query_key_value.weight', 'layers.23.attention.query_key_value.weight', 'layers.15.attention.bias', 'layers.31.input_layernorm.bias', 'layers.6.mlp.dense_4h_to_h.bias', 'layers.19.attention.query_key_value.bias', 'layers.23.input_layernorm.weight', 'layers.31.mlp.dense_4h_to_h.bias', 'layers.31.attention.masked_bias', 'layers.8.attention.dense.weight', 'layers.8.attention.query_key_value.bias', 'layers.19.attention.query_key_value.weight', 'layers.10.attention.masked_bias', 'layers.31.mlp.dense_4h_to_h.weight', 'embed_in.weight', 'layers.19.attention.masked_bias', 'layers.29.post_attention_layernorm.weight', 'layers.6.post_attention_layernorm.weight', 'layers.8.input_layernorm.bias', 'layers.17.attention.masked_bias', 'layers.0.attention.bias', 'layers.28.attention.bias', 'layers.4.attention.bias', 'layers.12.attention.masked_bias', 'layers.26.post_attention_layernorm.bias', 'layers.29.mlp.dense_4h_to_h.weight', 'layers.26.attention.dense.weight', 'layers.29.post_attention_layernorm.bias', 'layers.11.mlp.dense_h_to_4h.weight', 'layers.24.input_layernorm.weight', 'layers.13.attention.bias', 'layers.14.post_attention_layernorm.bias', 'layers.8.mlp.dense_4h_to_h.bias', 'layers.27.input_layernorm.bias', 'layers.10.mlp.dense_h_to_4h.bias', 'layers.19.mlp.dense_h_to_4h.weight', 'layers.17.mlp.dense_4h_to_h.bias', 'layers.8.attention.bias', 'layers.17.attention.rotary_emb.inv_freq', 'layers.20.attention.query_key_value.bias', 'layers.16.mlp.dense_4h_to_h.bias', 'layers.7.attention.dense.bias', 'layers.30.input_layernorm.weight', 'layers.7.input_layernorm.weight', 'layers.27.mlp.dense_h_to_4h.bias', 'layers.28.post_attention_layernorm.bias', 'layers.17.attention.dense.bias', 'layers.23.post_attention_layernorm.weight', 'layers.30.input_layernorm.bias', 'layers.2.attention.query_key_value.weight', 'layers.10.attention.dense.weight', 'layers.16.input_layernorm.bias', 'layers.19.input_layernorm.bias', 'layers.30.mlp.dense_4h_to_h.bias', 'layers.5.post_attention_layernorm.weight', 'layers.7.attention.dense.weight', 'layers.20.mlp.dense_4h_to_h.bias', 'layers.31.post_attention_layernorm.bias', 'layers.9.attention.bias', 'layers.15.mlp.dense_4h_to_h.weight', 'layers.21.attention.query_key_value.bias', 'layers.21.input_layernorm.weight', 'layers.9.input_layernorm.weight', 'layers.21.attention.bias', 'final_layer_norm.bias', 'layers.6.attention.query_key_value.weight', 'layers.26.input_layernorm.bias', 'layers.6.attention.query_key_value.bias', 'layers.14.attention.masked_bias', 'layers.16.post_attention_layernorm.bias', 'layers.10.mlp.dense_4h_to_h.bias', 'layers.11.attention.rotary_emb.inv_freq', 'layers.16.mlp.dense_h_to_4h.bias', 'layers.13.attention.query_key_value.weight', 'layers.25.attention.masked_bias', 'layers.3.post_attention_layernorm.weight', 'layers.12.attention.query_key_value.bias', 'layers.23.attention.query_key_value.bias', 'layers.21.post_attention_layernorm.weight', 'layers.16.mlp.dense_4h_to_h.weight', 'layers.21.input_layernorm.bias', 'layers.11.input_layernorm.weight', 'layers.29.input_layernorm.bias', 'layers.31.mlp.dense_h_to_4h.bias', 'layers.7.attention.query_key_value.weight', 'layers.18.attention.bias', 'layers.30.attention.bias', 'layers.20.mlp.dense_h_to_4h.bias', 'layers.3.input_layernorm.bias', 'layers.23.attention.masked_bias', 'layers.26.input_layernorm.weight', 'layers.2.attention.rotary_emb.inv_freq', 'layers.18.attention.query_key_value.bias', 'layers.2.post_attention_layernorm.bias', 'layers.14.attention.query_key_value.weight', 'layers.21.attention.query_key_value.weight', 'layers.13.mlp.dense_h_to_4h.bias', 'layers.5.input_layernorm.bias', 'layers.14.mlp.dense_4h_to_h.weight', 'layers.27.mlp.dense_h_to_4h.weight', 'layers.11.post_attention_layernorm.bias', 'layers.20.attention.masked_bias', 'layers.9.input_layernorm.bias', 'layers.6.attention.rotary_emb.inv_freq', 'layers.19.input_layernorm.weight', 'layers.7.post_attention_layernorm.bias', 'layers.18.attention.query_key_value.weight', 'layers.23.attention.dense.bias', 'layers.12.attention.query_key_value.weight', 'layers.23.input_layernorm.bias', 'layers.30.post_attention_layernorm.weight', 'layers.17.input_layernorm.weight', 'layers.8.attention.masked_bias', 'layers.5.attention.masked_bias', 'layers.21.mlp.dense_4h_to_h.weight', 'layers.22.attention.bias', 'layers.23.attention.bias', 'layers.9.attention.query_key_value.bias', 'layers.30.attention.rotary_emb.inv_freq', 'layers.15.attention.rotary_emb.inv_freq', 'layers.22.mlp.dense_4h_to_h.weight', 'layers.25.attention.query_key_value.weight', 'layers.28.post_attention_layernorm.weight', 'layers.16.attention.masked_bias', 'layers.22.attention.query_key_value.bias', 'layers.24.attention.dense.bias', 'layers.6.attention.masked_bias', 'layers.7.attention.query_key_value.bias', 'layers.4.mlp.dense_4h_to_h.bias', 'layers.25.attention.rotary_emb.inv_freq', 'layers.27.attention.rotary_emb.inv_freq', 'layers.6.post_attention_layernorm.bias', 'layers.15.input_layernorm.bias', 'layers.3.attention.dense.bias', 'layers.8.input_layernorm.weight', 'layers.1.mlp.dense_h_to_4h.bias', 'layers.30.mlp.dense_h_to_4h.weight', 'layers.17.attention.query_key_value.bias', 'layers.19.attention.dense.bias', 'layers.30.attention.masked_bias', 'layers.24.mlp.dense_4h_to_h.weight', 'layers.28.attention.masked_bias', 'layers.18.input_layernorm.weight', 'layers.20.attention.query_key_value.weight', 'layers.19.post_attention_layernorm.bias', 'layers.2.attention.masked_bias', 'layers.21.attention.masked_bias', 'layers.0.attention.dense.weight', 'layers.29.attention.dense.weight', 'layers.28.attention.rotary_emb.inv_freq', 'layers.14.attention.dense.bias', 'layers.6.attention.bias', 'layers.4.input_layernorm.weight', 'layers.1.attention.query_key_value.weight', 'layers.13.attention.masked_bias', 'layers.19.attention.rotary_emb.inv_freq', 'layers.20.post_attention_layernorm.weight', 'layers.3.attention.rotary_emb.inv_freq', 'layers.21.mlp.dense_h_to_4h.weight', 'layers.2.attention.bias', 'layers.30.attention.query_key_value.bias', 'layers.17.attention.dense.weight', 'layers.0.attention.dense.bias', 'layers.7.attention.masked_bias', 'layers.2.mlp.dense_h_to_4h.weight', 'layers.14.post_attention_layernorm.weight', 'layers.24.input_layernorm.bias', 'layers.3.mlp.dense_h_to_4h.bias', 'layers.12.attention.rotary_emb.inv_freq', 'layers.26.attention.rotary_emb.inv_freq', 'layers.20.post_attention_layernorm.bias', 'layers.7.attention.rotary_emb.inv_freq', 'layers.13.post_attention_layernorm.bias', 'layers.27.post_attention_layernorm.bias', 'layers.4.post_attention_layernorm.bias', 'layers.10.input_layernorm.bias', 'layers.31.mlp.dense_h_to_4h.weight', 'layers.6.mlp.dense_4h_to_h.weight', 'layers.4.attention.dense.weight', 'layers.29.attention.rotary_emb.inv_freq', 'layers.0.mlp.dense_4h_to_h.bias', 'layers.4.attention.query_key_value.weight', 'layers.24.attention.dense.weight', 'layers.27.attention.dense.bias', 'layers.9.mlp.dense_h_to_4h.weight', 'layers.6.attention.dense.bias', 'layers.9.mlp.dense_h_to_4h.bias', 'layers.2.input_layernorm.bias', 'layers.28.mlp.dense_h_to_4h.bias', 'layers.8.post_attention_layernorm.bias', 'layers.14.mlp.dense_h_to_4h.weight', 'layers.10.mlp.dense_4h_to_h.weight', 'layers.1.mlp.dense_h_to_4h.weight', 'layers.23.mlp.dense_h_to_4h.weight', 'layers.0.post_attention_layernorm.bias', 'layers.5.post_attention_layernorm.bias', 'layers.20.input_layernorm.bias', 'layers.10.post_attention_layernorm.bias', 'layers.24.mlp.dense_h_to_4h.weight', 'layers.30.mlp.dense_4h_to_h.weight', 'layers.13.mlp.dense_4h_to_h.weight', 'layers.3.post_attention_layernorm.bias', 'layers.20.attention.dense.weight', 'layers.4.attention.masked_bias', 'layers.13.attention.dense.weight', 'layers.27.attention.bias', 'layers.3.attention.bias', 'layers.28.attention.dense.weight', 'layers.2.attention.dense.bias', 'layers.21.post_attention_layernorm.bias', 'layers.29.attention.masked_bias', 'layers.27.input_layernorm.weight', 'layers.1.post_attention_layernorm.bias', 'layers.15.attention.query_key_value.weight', 'layers.1.attention.dense.bias', 'layers.11.attention.masked_bias', 'layers.4.attention.dense.bias', 'layers.14.attention.bias', 'layers.3.mlp.dense_4h_to_h.bias', 'layers.0.post_attention_layernorm.weight', 'layers.18.attention.rotary_emb.inv_freq', 'layers.22.attention.rotary_emb.inv_freq', 'layers.7.mlp.dense_4h_to_h.weight', 'layers.10.input_layernorm.weight', 'layers.6.mlp.dense_h_to_4h.bias', 'layers.11.attention.bias', 'layers.13.mlp.dense_h_to_4h.weight', 'layers.0.attention.rotary_emb.inv_freq', 'layers.8.attention.dense.bias', 'layers.28.attention.query_key_value.bias', 'layers.8.post_attention_layernorm.weight', 'layers.15.attention.dense.bias', 'layers.4.attention.query_key_value.bias', 'layers.28.attention.query_key_value.weight', 'layers.7.input_layernorm.bias', 'layers.1.input_layernorm.bias', 'layers.10.attention.bias', 'layers.6.input_layernorm.bias', 'layers.16.attention.bias', 'layers.1.attention.query_key_value.bias', 'layers.27.attention.dense.weight', 'layers.10.attention.dense.bias', 'layers.7.mlp.dense_h_to_4h.bias', 'layers.16.attention.dense.bias', 'layers.18.post_attention_layernorm.bias', 'layers.27.post_attention_layernorm.weight', 'layers.26.post_attention_layernorm.weight', 'layers.26.attention.query_key_value.weight', 'layers.3.mlp.dense_4h_to_h.weight', 'layers.22.attention.masked_bias', 'layers.28.mlp.dense_h_to_4h.weight', 'layers.18.mlp.dense_4h_to_h.weight', 'layers.17.post_attention_layernorm.bias', 'layers.2.mlp.dense_4h_to_h.weight', 'layers.12.post_attention_layernorm.bias', 'layers.0.attention.masked_bias', 'layers.24.attention.query_key_value.weight', 'layers.16.attention.dense.weight', 'layers.19.mlp.dense_h_to_4h.bias', 'layers.5.attention.query_key_value.bias', 'layers.1.attention.masked_bias', 'layers.31.attention.bias', 'layers.6.mlp.dense_h_to_4h.weight', 'layers.31.attention.rotary_emb.inv_freq', 'layers.21.mlp.dense_4h_to_h.bias', 'layers.1.attention.dense.weight', 'layers.2.post_attention_layernorm.weight', 'layers.12.input_layernorm.weight', 'layers.17.mlp.dense_h_to_4h.weight', 'layers.4.input_layernorm.bias', 'layers.20.mlp.dense_4h_to_h.weight', 'layers.25.mlp.dense_4h_to_h.weight', 'layers.8.attention.rotary_emb.inv_freq', 'layers.14.mlp.dense_4h_to_h.bias', 'layers.9.attention.masked_bias', 'layers.26.attention.query_key_value.bias', 'layers.17.attention.query_key_value.weight', 'layers.15.attention.masked_bias', 'layers.9.attention.rotary_emb.inv_freq', 'layers.25.attention.query_key_value.bias', 'layers.17.mlp.dense_h_to_4h.bias', 'layers.29.input_layernorm.weight', 'layers.12.mlp.dense_h_to_4h.weight', 'layers.4.mlp.dense_h_to_4h.weight', 'layers.16.post_attention_layernorm.weight', 'layers.8.attention.query_key_value.weight', 'layers.12.mlp.dense_h_to_4h.bias', 'layers.14.attention.rotary_emb.inv_freq', 'layers.26.mlp.dense_4h_to_h.weight', 'layers.16.attention.query_key_value.weight', 'layers.23.post_attention_layernorm.bias', 'layers.11.attention.dense.weight', 'layers.25.post_attention_layernorm.weight', 'layers.11.post_attention_layernorm.weight', 'layers.27.attention.query_key_value.weight', 'layers.25.input_layernorm.weight', 'layers.8.mlp.dense_h_to_4h.weight', 'layers.26.attention.dense.bias', 'layers.29.attention.bias', 'layers.0.attention.query_key_value.weight', 'layers.5.mlp.dense_4h_to_h.weight', 'layers.12.mlp.dense_4h_to_h.weight', 'layers.2.mlp.dense_4h_to_h.bias', 'layers.23.attention.dense.weight', 'layers.22.input_layernorm.weight', 'layers.0.mlp.dense_4h_to_h.weight', 'layers.20.input_layernorm.weight', 'layers.18.attention.masked_bias', 'layers.13.mlp.dense_4h_to_h.bias', 'layers.3.mlp.dense_h_to_4h.weight', 'layers.18.attention.dense.bias', 'layers.17.input_layernorm.bias', 'layers.26.mlp.dense_h_to_4h.weight', 'layers.9.attention.query_key_value.weight', 'layers.12.attention.bias', 'layers.28.attention.dense.bias', 'layers.22.post_attention_layernorm.weight', 'layers.9.post_attention_layernorm.weight', 'layers.17.mlp.dense_4h_to_h.weight', 'layers.21.attention.rotary_emb.inv_freq', 'layers.15.attention.query_key_value.bias', 'layers.18.mlp.dense_h_to_4h.weight', 'layers.1.post_attention_layernorm.weight', 'layers.5.mlp.dense_h_to_4h.weight', 'layers.8.mlp.dense_h_to_4h.bias', 'layers.16.mlp.dense_h_to_4h.weight', 'layers.26.attention.masked_bias', 'layers.5.input_layernorm.weight', 'layers.18.mlp.dense_4h_to_h.bias', 'layers.15.mlp.dense_4h_to_h.bias', 'layers.0.attention.query_key_value.bias', 'layers.11.mlp.dense_4h_to_h.weight', 'layers.18.mlp.dense_h_to_4h.bias', 'layers.23.mlp.dense_h_to_4h.bias', 'layers.26.mlp.dense_h_to_4h.bias', 'layers.0.input_layernorm.weight', 'layers.12.mlp.dense_4h_to_h.bias', 'layers.6.input_layernorm.weight', 'layers.18.input_layernorm.bias', 'layers.5.attention.bias', 'layers.23.mlp.dense_4h_to_h.weight', 'layers.2.attention.query_key_value.bias', 'layers.12.attention.dense.weight', 'layers.30.attention.dense.weight', 'layers.31.attention.dense.weight', 'layers.4.attention.rotary_emb.inv_freq', 'layers.11.mlp.dense_4h_to_h.bias', 'layers.7.post_attention_layernorm.weight', 'layers.19.post_attention_layernorm.weight', 'layers.31.attention.dense.bias', 'layers.22.mlp.dense_4h_to_h.bias', 'layers.4.post_attention_layernorm.weight', 'layers.5.attention.dense.weight', 'layers.4.mlp.dense_h_to_4h.bias', 'layers.12.input_layernorm.bias', 'embed_out.weight', 'layers.19.mlp.dense_4h_to_h.weight', 'layers.29.mlp.dense_h_to_4h.weight', 'final_layer_norm.weight', 'layers.18.post_attention_layernorm.weight', 'layers.11.attention.query_key_value.weight', 'layers.15.input_layernorm.weight', 'layers.9.post_attention_layernorm.bias', 'layers.16.attention.rotary_emb.inv_freq', 'layers.13.attention.query_key_value.bias', 'layers.15.post_attention_layernorm.bias', 'layers.29.mlp.dense_4h_to_h.bias', 'layers.15.post_attention_layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
0it [00:00, ?it/s]1it [00:02,  2.59s/it]2it [00:03,  1.53s/it]3it [00:03,  1.07s/it]4it [00:04,  1.44it/s]5it [00:04,  1.52it/s]6it [00:06,  1.16s/it]7it [00:07,  1.03it/s]8it [00:07,  1.32it/s]9it [00:08,  1.44it/s]10it [00:08,  1.50it/s]11it [00:09,  1.77it/s]12it [00:09,  1.84it/s]13it [00:09,  2.21it/s]14it [00:10,  1.69it/s]15it [00:15,  1.71s/it]16it [00:15,  1.28s/it]17it [00:15,  1.05s/it]18it [00:21,  2.35s/it]19it [00:21,  1.83s/it]20it [00:21,  1.32s/it]5121it [00:22, 1606.28it/s]11536it [00:22, 4296.31it/s]17410it [00:22, 7448.31it/s]22986it [00:22, 11132.50it/s]28960it [00:22, 15912.40it/s]33802it [00:22, 19854.07it/s]39567it [00:22, 25494.36it/s]45552it [00:22, 31524.92it/s]51490it [00:22, 37111.60it/s]57686it [00:22, 42639.95it/s]63451it [00:23, 46260.15it/s]69191it [00:23, 48552.75it/s]74877it [00:23, 50758.26it/s]80549it [00:23, 52307.09it/s]86235it [00:23, 53574.90it/s]91902it [00:23, 52669.21it/s]98149it [00:23, 55448.89it/s]104176it [00:23, 56839.39it/s]110118it [00:23, 57573.63it/s]116251it [00:24, 58676.88it/s]122183it [00:24, 55366.75it/s]128065it [00:24, 56347.76it/s]133763it [00:24, 55077.83it/s]140049it [00:24, 57306.91it/s]146009it [00:24, 57968.99it/s]152257it [00:24, 59252.28it/s]158208it [00:24, 57627.93it/s]163997it [00:24, 56994.84it/s]169715it [00:24, 56511.12it/s]176320it [00:25, 59290.44it/s]182450it [00:25, 59880.05it/s]188451it [00:25, 59366.01it/s]194397it [00:25, 58669.69it/s]200272it [00:25, 58314.23it/s]206470it [00:25, 59392.81it/s]212415it [00:25, 59396.72it/s]214670it [00:25, 8353.07it/s] 
num docs - i:  0
num docs - i:  1
num docs - i:  2
num docs - i:  3
num docs - i:  4
num docs - i:  5
num docs - i:  6
num docs - i:  7
num docs - i:  8
num docs - i:  9
num docs - i:  10
num docs - i:  11
num docs - i:  12
num docs - i:  13
num docs - i:  14
num docs - i:  15
num docs - i:  16
num docs - i:  17
num docs - i:  18
num docs - i:  19
Done calculations!
Saved!
