{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3676a4cf-c972-4d46-b043-d4f671fd2ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import submitit\n",
    "import os\n",
    "from fastchat.model import get_conversation_template\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a4154f0-d926-4ff9-9aea-7a9116ae50e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# creative prompts\n",
    "\n",
    "creative_prompts = [\"Write a poem\", \"Tell me a joke\", \"Describe the feeling of love\", \"Write a story starting with 'Once upon a time...'\",\n",
    "                    \"Tell a story about a dog\", \"Write a song\", \"Write a poem about a robot\", \"Invent an original recipe\",\n",
    "                    \"Imagine a new object and describe what it looks like.\", \"Imagine a new philosophy and describe it.\", \n",
    "                    \"Create a new game and explain the rules.\", \"Write a new myth explaining the origin of rainbows.\", \n",
    "                    \"Write a dialogue between the moon and the sun\", \"Compose a lullaby\", \"Write a news headline for the year 2050.\",\n",
    "                    \"Invent a riddle and write it down.\", \"Write a story about two people seeing each other for the first time.\",\n",
    "                    \"Write a story about a person who is afraid of the dark.\", \"Make a new pun about llamas.\", \"Invent a new word and define it.\"]\n",
    "\n",
    "# factual prompts\n",
    "factual_prompts = [\"What is the capital of France?\", \"How is H2O commonly known?\", \"What is the largest country in the world?\", \"How many days are in a year?\",\n",
    "                   \"What is the largest planet in the solar system?\", \"What is the largest animal in the world?\", \"How do you say hello in Spanish?\", \"Who won the 2018 World Cup?\",\n",
    "                   \"What is the biggest city in Europe?\", \"What is the largest country in Africa?\", \"What was the last battle of Napoleon?\", \"How do you call someone from New Zealand?\",\n",
    "                   \"How do you call someone who studies plants?\", \"Who invented the telephone?\", \"What mammal lays eggs?\", \"Which bone is the longest in the human body?\", \"What is the anthem of France?\",\n",
    "                   \"Who wrote Cannery Row?\", \"Who was the first president of the United States?\", \"Which painter painted the Mona Lisa?\"]\n",
    "\n",
    "\n",
    "\n",
    "# factual prompts whose answers are longer\n",
    "# not used right now\n",
    "factual_prompts_longer = [\"What separates prehistory from history?\", \"How were hyerogliphics deciphered?\"]\n",
    "\n",
    "print(len(creative_prompts))\n",
    "print(len(factual_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6173c5e1-f2db-4293-808a-11dd664ba1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/llama\n",
    "# https://github.com/facebookresearch/llama/blob/main/llama/generation.py \n",
    "# chat_completion style for llama2-chat\n",
    "# https://github.com/facebookresearch/llama/blob/main/example_chat_completion.py \n",
    "def format_prompt_llama2_chat(prompt):\n",
    "    prompt_format = \"\"\"<s>[INST] <<SYS>>\n",
    "    You are a helpful, respectful and honest assistant. Always answer without asking questions or clarifications.\n",
    "    <</SYS>>\n",
    "\n",
    "    {} [/INST]\"\"\"\n",
    "    return prompt_format.format(prompt)\n",
    "\n",
    "# https://arxiv.org/abs/2204.05862\n",
    "# https://huggingface.co/datasets/Anthropic/hh-rlhf\n",
    "# https://huggingface.co/datasets/Dahoas/static-hh\n",
    "def format_prompt_pythia_helpful(prompt):\n",
    "    prompt_format = \"\"\"Human: {} Assistant: \"\"\"\n",
    "    return prompt_format.format(prompt)\n",
    "\n",
    "def format_prompt_PLM(prompt):\n",
    "    prompt_format = \"\"\"{} Okay, here goes: \"\"\"\n",
    "    return prompt_format.format(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4142c7c9-214f-49de-a3c9-31a3ea07dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [k / 10. for k in range(1, 16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52bbe3fd-bcf5-47b7-8f6c-69d044ff9b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c427395fcb44498eb467e473e26e2bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 10.75 GiB of which 68.44 MiB is free. Process 3977277 has 1.79 GiB memory in use. Process 4034090 has 8.86 GiB memory in use. Of the allocated memory 8.71 GiB is allocated by PyTorch, and 1.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m model_completions_creative \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(creative_prompts): \n\u001b[0;32m---> 86\u001b[0m     model_completions \u001b[38;5;241m=\u001b[39m generate_samples(prompt, temperatures, model)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t_index, completion \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(model_completions):\n\u001b[1;32m     88\u001b[0m         completions_creative[t_index, i, models\u001b[38;5;241m.\u001b[39mindex(model)] \u001b[38;5;241m=\u001b[39m completion\n",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m, in \u001b[0;36mgenerate_samples\u001b[0;34m(prompt, temperatures, model_name)\u001b[0m\n\u001b[1;32m     16\u001b[0m     full_prompt \u001b[38;5;241m=\u001b[39m format_prompt_llama2_chat(prompt)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# elif model_name == \"vicuna1.5\":\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     model_path = \"lmsys/vicuna-7b-v1.5\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     tokenizer = AutoTokenizer.from_pretrained(model_path)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     model = AutoModelForCausalLM.from_pretrained(model_path)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#     #adapted from https://github.com/lm-sys/FastChat/blob/a47b8f9e93c8b5a85e81d1ae33e3a1106d8cdf80/fastchat/serve/huggingface_api.py\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#     full_prompt = format_prompt_vicuna(prompt)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(full_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m completions \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/analyse-llms/lib/python3.11/site-packages/transformers/modeling_utils.py:2460\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2457\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2458\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2459\u001b[0m         )\n\u001b[0;32m-> 2460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/analyse-llms/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/analyse-llms/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/analyse-llms/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/analyse-llms/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/analyse-llms/lib/python3.11/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/analyse-llms/lib/python3.11/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 10.75 GiB of which 68.44 MiB is free. Process 3977277 has 1.79 GiB memory in use. Process 4034090 has 8.86 GiB memory in use. Of the allocated memory 8.71 GiB is allocated by PyTorch, and 1.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# each temperature and for each prompt, generate n_generations samples\n",
    "temperatures = [k / 10. for k in range(1, 16)]\n",
    "models = [\"llama2-chat\"]\n",
    "n_generations = 25\n",
    "completions_creative = np.zeros((len(temperatures), len(creative_prompts), len(models)), dtype=object)\n",
    "completions_factual = np.zeros((len(temperatures), len(factual_prompts), len(models)), dtype=object)\n",
    "\n",
    "# define the function to be submitted\n",
    "# def generate_samples(args):\n",
    "def generate_samples(prompt, temperatures, model_name):\n",
    "    max_return_sequences = 5 #for memory reasons, we generate the samples in batches of 5\n",
    "    # i, prompt, temperatures, model_name = args\n",
    "    if model_name == \"llama2-chat\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.bfloat16 )\n",
    "        full_prompt = format_prompt_llama2_chat(prompt)\n",
    "    # elif model_name == \"vicuna1.5\":\n",
    "    #     model_path = \"lmsys/vicuna-7b-v1.5\"\n",
    "    #     tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    #     model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    #     #adapted from https://github.com/lm-sys/FastChat/blob/a47b8f9e93c8b5a85e81d1ae33e3a1106d8cdf80/fastchat/serve/huggingface_api.py\n",
    "    #     full_prompt = format_prompt_vicuna(prompt)\n",
    "    model.to(\"cuda:0\")\n",
    "    input_ids = tokenizer.encode(full_prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    completions = []\n",
    "    for temperature in temperatures:\n",
    "        temp_completions = []\n",
    "        for _ in range(n_generations // max_return_sequences):\n",
    "            samples = model.generate(input_ids, temperature=temperature, max_length=input_ids.shape[1] + 70,\n",
    "                                    num_return_sequences=max_return_sequences, do_sample=True)\n",
    "            # remove prompt from the samples\n",
    "            samples = [sample[input_ids.shape[1]:] for sample in samples]\n",
    "            samples = [tokenizer.decode(sample, skip_special_tokens=True) for sample in samples]\n",
    "            temp_completions.extend(samples)\n",
    "        completions.append(temp_completions)\n",
    "    return completions\n",
    "\n",
    "# create a folder for the logs of the submitted jobs\n",
    "# os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    # create a submitit executor\n",
    "    # executor = submitit.AutoExecutor(folder=\"logs\")\n",
    "\n",
    "    # specify the parameters for the Slurm job\n",
    "    #exclude margpu002 and margpu003\n",
    "    # executor.update_parameters(timeout_min=60, slurm_partition=\"parietal,gpu,normal\", gpus_per_node=1,\n",
    "    #                             # exclude nodes\n",
    "    #                             exclude=\"margpu002,margpu003\")\n",
    "    \n",
    "    # For creative prompts\n",
    "    # args_list_creative = [(i, prompt, temperatures, model) for i, prompt in enumerate(creative_prompts)]\n",
    "    # jobs_creative = executor.map_array(generate_samples, args_list_creative)\n",
    "\n",
    "    # # For factual prompts\n",
    "    # args_list_factual = [(i, prompt, temperatures, model) for i, prompt in enumerate(factual_prompts)]\n",
    "    # jobs_factual = executor.map_array(generate_samples, args_list_factual)\n",
    "\n",
    "    # # # Collect the results for creative prompts\n",
    "    # for i, job in enumerate(jobs_creative):\n",
    "    #     model_completions = job.result()\n",
    "    #     for t_index, completion in enumerate(model_completions):\n",
    "    #         completions_creative[t_index, i, models.index(model)] = completion\n",
    "        \n",
    "    # # # Collect the results for factual prompts\n",
    "    # for i, job in enumerate(jobs_factual):\n",
    "    #     model_completions = job.result()\n",
    "    #     for t_index, completion in enumerate(model_completions):\n",
    "    #         completions_factual[t_index, i, models.index(model)] = completion\n",
    "\n",
    "    # # Collect the results for creative prompts\n",
    "    # for i, job in enumerate(jobs_creative):\n",
    "    #     model_completions = job.result()\n",
    "    #     for t_index, completion in enumerate(model_completions):\n",
    "    #         completions_creative[t_index, i, models.index(model)] = completion\n",
    "\n",
    "    # # Collect the results for factual prompts\n",
    "    # for i, job in enumerate(jobs_factual):\n",
    "    #     model_completions = job.result()\n",
    "    #     for t_index, completion in enumerate(model_completions):\n",
    "    #         completions_factual[t_index, i, models.index(model)] = completion\n",
    "\n",
    "    model_completions_creative = []\n",
    "    for i, prompt in enumerate(creative_prompts): \n",
    "        model_completions = generate_samples(prompt, temperatures, model)\n",
    "        for t_index, completion in enumerate(model_completions):\n",
    "            completions_creative[t_index, i, models.index(model)] = completion\n",
    "\n",
    "    model_completions_factual = []\n",
    "    for i, prompt in enumerate(creative_prompts): \n",
    "        model_completions = generate_samples(prompt, temperatures, model)\n",
    "        for t_index, completion in enumerate(model_completions):\n",
    "            completions_factual[t_index, i, models.index(model)] = completion\n",
    "\n",
    "# Save the results\n",
    "np.save(f'{model}_completions_creative_max_length70.npy', completions_creative)\n",
    "np.save(f'{model}_completions_factual_max_length70.npy', completions_factual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ce4790a-1bbc-4ad9-a17a-7b7ee497da65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  'Write a poem',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (1,\n",
       "  'Tell me a joke',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (2,\n",
       "  'Describe the feeling of love',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (3,\n",
       "  \"Write a story starting with 'Once upon a time...'\",\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (4,\n",
       "  'Tell a story about a dog',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (5,\n",
       "  'Write a song',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (6,\n",
       "  'Write a poem about a robot',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (7,\n",
       "  'Invent an original recipe',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (8,\n",
       "  'Imagine a new object and describe what it looks like.',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (9,\n",
       "  'Imagine a new philosophy and describe it.',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (10,\n",
       "  'Create a new game and explain the rules.',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (11,\n",
       "  'Write a new myth explaining the origin of rainbows.',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (12,\n",
       "  'Write a dialogue between the moon and the sun',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (13,\n",
       "  'Compose a lullaby',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (14,\n",
       "  'Write a news headline for the year 2050.',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (15,\n",
       "  'Invent a riddle and write it down.',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (16,\n",
       "  'Write a story about two people seeing each other for the first time.',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (17,\n",
       "  'Write a story about a person who is afraid of the dark.',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (18,\n",
       "  'Make a new pun about llamas.',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat'),\n",
       " (19,\n",
       "  'Invent a new word and define it.',\n",
       "  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "  'llama2-chat')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args_list_creative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0641aa8-d5de-4e03-9753-93fbc7ea297c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LocalJob<job_id=2239679, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2239683, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2239724, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2239765, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2239806, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2239847, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2239891, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2239932, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2239973, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2240015, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2240066, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2240107, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2240151, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2240191, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2240233, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2240279, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2240320, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2240361, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2240404, task_id=0, state=\"FINISHED\">,\n",
       " LocalJob<job_id=2240445, task_id=0, state=\"FINISHED\">]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_creative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a816f31d-a092-4271-a7f4-188d71147b78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
